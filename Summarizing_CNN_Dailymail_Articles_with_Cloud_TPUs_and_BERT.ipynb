{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summarizing CNN/Dailymail Articles with Cloud TPUs and BERT",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clarencechen/cnn-dailymail-summary/blob/master/Summarizing_CNN_Dailymail_Articles_with_Cloud_TPUs_and_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "bfJp0Q0M1npS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "metadata": {
        "id": "rxkNIKFM1vWx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "edfbxDDh2AEs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Summarizing CNN/Dailymail Articles with Cloud TPUs and BERT\n"
      ]
    },
    {
      "metadata": {
        "id": "wYp6XVQU2POn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        " <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "KRQ6Fjra3Ruq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download data\n",
        "\n",
        "Download *The Complete Works of William Shakespeare* as a single text file from [Project Gutenberg](https://www.gutenberg.org/). We'll use snippets from this file as the *training data* for the model. The *target* snippet is offset by one character."
      ]
    },
    {
      "metadata": {
        "id": "j8sIXh1DEDDd",
        "colab_type": "code",
        "outputId": "d2419def-1cb7-4184-dea0-95f49070d61c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "    # Upload credentials to TPU.\n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "        auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    # Now credentials are set for all future sessions on this TPU.\n",
        "    print('TPU devices:')\n",
        "    print(session.list_devices())\n",
        "\n",
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "OUTPUT_DIR = '/content/model/'\n",
        "tf.logging.set_verbosity(tf.logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.89.239.2:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 11582478840878583954), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 9952103659584718622), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 18033498473482104438), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 14319539187879693053), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 9675666561288967995), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 8125228641297061490), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 8601934407541013928), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 12754932271768098734), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 17481024439653843099), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 17941769384395501605), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 17191320602847230219), _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 8385697350496404269)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bbb05dNynDrQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "The model is defined as a two-layer, forward-LSTMâ€”with two changes from the `tf.keras` standard LSTM definition:\n",
        "\n",
        "1. Define the input `shape` of our model which satisfies the [XLA compiler](https://www.tensorflow.org/performance/xla/)'s static shape requirement.\n",
        "2. Use `tf.train.Optimizer` instead of a standard Keras optimizer (Keras optimizer support is still experimental)."
      ]
    },
    {
      "metadata": {
        "id": "PJiYnBNgPSzM",
        "colab_type": "code",
        "outputId": "11299885-fe54-4e5b-cbfe-a6a11438a7c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "if not 'bert_repo' in sys.path:\n",
        "    sys.path += ['bert_repo']\n",
        "\n",
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "BERT_MODEL = 'cased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "BERT_PRETRAINED_DIR = 'gs://cloud-tpu-checkpoints/bert/' + BERT_MODEL\n",
        "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n",
        "!gsutil ls $BERT_PRETRAINED_DIR"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** BERT pretrained directory: gs://cloud-tpu-checkpoints/bert/cased_L-12_H-768_A-12 *****\n",
            "gs://cloud-tpu-checkpoints/bert/cased_L-12_H-768_A-12/bert_config.json\n",
            "gs://cloud-tpu-checkpoints/bert/cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001\n",
            "gs://cloud-tpu-checkpoints/bert/cased_L-12_H-768_A-12/bert_model.ckpt.index\n",
            "gs://cloud-tpu-checkpoints/bert/cased_L-12_H-768_A-12/bert_model.ckpt.meta\n",
            "gs://cloud-tpu-checkpoints/bert/cased_L-12_H-768_A-12/checkpoint\n",
            "gs://cloud-tpu-checkpoints/bert/cased_L-12_H-768_A-12/vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VzBYDJI0_Tfm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset download"
      ]
    },
    {
      "metadata": {
        "id": "AbL6cqCl7hnt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Convert the Shakespeare Corpus into Newline-separated Sentences"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gTk-04y3oTyx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!test -d cnn || mkdir cnn/\n",
        "!test -d dailymail || mkdir dailymail/\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download the story files from using their Drive file ID.\n",
        "CNN_DRIVE_ID, DAILYMAIL_DRIVE_ID = '0BwmD_VLjROrfTHk4NFg2SndKcjQ', '0BwmD_VLjROrfM1BxdkxVaTY2bWs'\n",
        "CNN_DIR, DAILYMAIL_DIR, URL_DIR = '/content/cnn/', '/content/dailymail/', '/content/url_lists/'\n",
        "TGZ_NAME = 'stories.tgz'\n",
        "CNN_TGZ, DAILYMAIL_TGZ = CNN_DIR + TGZ_NAME, DAILYMAIL_DIR + TGZ_NAME\n",
        "\n",
        "!test -d $CNN_DIR || mkdir $CNN_DIR\n",
        "!test -d $URL_DIR || mkdir $URL_DIR\n",
        "!test -d $DAILYMAIL_DIR || mkdir $DAILYMAIL_DIR\n",
        "\n",
        "cnn, dailymail = drive.CreateFile({'id': CNN_DRIVE_ID}), drive.CreateFile({'id': DAILYMAIL_DRIVE_ID})\n",
        "cnn.GetContentFile(CNN_TGZ)\n",
        "dailymail.GetContentFile(DAILYMAIL_TGZ)\n",
        "\n",
        "!tar -xzf $CNN_TGZ\n",
        "!tar -xzf $DAILYMAIL_TGZ\n",
        "\n",
        "!rm $CNN_TGZ\n",
        "!rm $DAILYMAIL_TGZ\n",
        "\n",
        "# Download URL lists of stories for each set using wget\n",
        "!wget --secure-protocol=auto https://raw.githubusercontent.com/becxer/cnn-dailymail/master/url_lists/all_val.txt\n",
        "!wget --secure-protocol=auto https://raw.githubusercontent.com/becxer/cnn-dailymail/master/url_lists/all_train.txt\n",
        "!wget --secure-protocol=auto https://raw.githubusercontent.com/becxer/cnn-dailymail/master/url_lists/all_test.txt\n",
        "!mv all_val.txt all_train.txt all_test.txt $URL_DIR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0FDVYo8GH7vn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset Preprocessing\n",
        "Now that all the news stories have been downloaded, they need to be tokenized and batched in order for BERT to compute their embeddings per sentence.  We will only extract the activations from the last layer. "
      ]
    },
    {
      "metadata": {
        "id": "ar-mgkH3NOwJ",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "# Define directory to store TFRecord data and final results (must be Google Cloud Bucket or BERT cannoy run) \n",
        "FINISHED_DIR = \"finished_files\" #@param {type:\"string\"}\n",
        "CHUNKED_DIR = os.path.join(FINISHED_DIR, \"chunked\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4qXJD21tOAP2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "import collections\n",
        "import json\n",
        "import re\n",
        "import hashlib\n",
        "\n",
        "import modeling\n",
        "import tokenization\n",
        "\n",
        "class InputExample(object):\n",
        "\t\"\"\"A single story with its article text and highlights.\"\"\"\n",
        "\n",
        "\tdef __init__(self, story_id, article, highlights):\n",
        "\t\tself.story_id = story_id\n",
        "\t\tself.article = article\n",
        "\t\tself.highlights = highlights\n",
        "\n",
        "class InputFeatures(object):\n",
        "\t\"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "\tdef __init__(self, unique_id, example_id, doc_span_index, tokens, token_is_max_context, input_ids, input_mask, input_type_ids, is_real=True):\n",
        "\t\tself.unique_id = unique_id\n",
        "\t\tself.tokens = tokens\n",
        "\t\tself.example_id = example_id,\n",
        "\t\tself.doc_span_index = doc_span_index,\n",
        "\t\tself.token_is_max_context = token_is_max_context,\n",
        "\t\tself.input_ids = input_ids\n",
        "\t\tself.input_mask = input_mask\n",
        "\t\tself.input_type_ids = input_type_ids\n",
        "\t\tself.is_real = is_real\n",
        "\n",
        "def file_based_input_fn_builder(input_file, seq_length, num_cores):\n",
        "\t\"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "\tname_to_features = {\n",
        "\t\t\t\"unique_id\": tf.FixedLenFeature([], tf.int64),\n",
        "\t\t\t\"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "\t\t\t\"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "\t\t\t\"input_type_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "\t\t\t}\n",
        "\n",
        "\tdef _decode_record(record, name_to_features):\n",
        "\t\t\"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "\t\tfeature_set = tf.parse_single_example(record, name_to_features)\n",
        "\n",
        "\t\t# tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
        "\t\t# So cast all int64 to int32.\n",
        "\t\tfor name in list(feature_set.keys()):\n",
        "\t\t\tt = feature_set[name]\n",
        "\t\t\tif t.dtype == tf.int64:\n",
        "\t\t\t\tt = tf.to_int32(t)\n",
        "\t\t\tfeature_set[name] = t\n",
        "\n",
        "\t\treturn feature_set\n",
        "\n",
        "\tdef input_fn(params):\n",
        "\t\t\"\"\"The actual input function.\"\"\"\n",
        "\t\tbatch_size = params[\"batch_size\"]\n",
        "\n",
        "\t\t# For training, we want a lot of parallel reading and shuffling.\n",
        "\t\t# For eval, we want no shuffling and parallel reading doesn't matter.\n",
        "\t\td = tf.data.TFRecordDataset(input_file, buffer_size=16*1024*1024)\n",
        "\n",
        "\t\td = d.apply(\n",
        "\t\t\t\ttf.contrib.data.map_and_batch(\n",
        "\t\t\t\t\tlambda record: _decode_record(record, name_to_features),\n",
        "\t\t\t\t\tbatch_size=batch_size,\n",
        "\t\t\t\t\tnum_parallel_batches=num_cores,\n",
        "\t\t\t\t\tdrop_remainder=False))\n",
        "\t\t# Pefetch data while training\n",
        "\t\td = d.prefetch(tf.contrib.data.AUTOTUNE)\n",
        "\n",
        "\t\treturn d\n",
        "\n",
        "\treturn input_fn\n",
        "\n",
        "def model_fn_builder(bert_config, init_checkpoint, layer_indexes, use_tpu=True, use_one_hot_embeddings=True):\n",
        "\t\"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "\tdef model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "\t\t\"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "        \n",
        "\t\ttf.logging.info(\"*** Features ***\")\n",
        "\t\tfor name in sorted(features.keys()):\n",
        "\t\t\ttf.logging.info(\"  name = {}, shape = {}\".format(name, features[name].shape))\n",
        "\n",
        "\t\tunique_id = features[\"unique_id\"]\n",
        "\t\tinput_ids = features[\"input_ids\"]\n",
        "\t\tinput_mask = features[\"input_mask\"]\n",
        "\t\tinput_type_ids = features[\"input_type_ids\"]\n",
        "\n",
        "\t\tmodel = modeling.BertModel(\n",
        "\t\t\t\tconfig=bert_config,\n",
        "\t\t\t\tis_training=False,\n",
        "\t\t\t\tinput_ids=input_ids,\n",
        "\t\t\t\tinput_mask=input_mask,\n",
        "\t\t\t\ttoken_type_ids=input_type_ids,\n",
        "\t\t\t\tuse_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "\t\tif mode != tf.estimator.ModeKeys.PREDICT:\n",
        "\t\t\traise ValueError(\"Only PREDICT modes are supported: %s\" % (mode))\n",
        "\n",
        "\t\ttvars = tf.trainable_variables()\n",
        "\t\tscaffold_fn = None\n",
        "\t\t(assignment_map, initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(\n",
        "\t\t\t\ttvars, init_checkpoint)\n",
        "\t\t\n",
        "\t\tif use_tpu:\n",
        "\t\t\tdef tpu_scaffold():\n",
        "\t\t\t\ttf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\t\t\t\treturn tf.train.Scaffold()\n",
        "\t\t\tscaffold_fn = tpu_scaffold\n",
        "\t\telse:\n",
        "\t\t\ttf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "\t\ttf.logging.info(\"**** Trainable Variables ****\")\n",
        "\t\tfor var in tvars:\n",
        "\t\t\tinit_string = \"\"\n",
        "\t\t\tif var.name in initialized_variable_names:\n",
        "\t\t\t\tinit_string = \", *INIT_FROM_CKPT*\"\n",
        "\t\t\ttf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape, init_string)\n",
        "\n",
        "\t\tall_layers = model.get_all_encoder_layers()\n",
        "\n",
        "\t\tpredictions = {\n",
        "\t\t\t\t\"unique_id\": unique_id,\n",
        "\t\t}\n",
        "\n",
        "\t\tfor (i, layer_index) in enumerate(layer_indexes):\n",
        "\t\t\tpredictions['layer_output_{:d}'.format(i)] = all_layers[layer_index]\n",
        "\n",
        "\t\toutput_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "\t\t\t\tmode=mode, predictions=predictions, scaffold_fn=scaffold_fn)\n",
        "\t\treturn output_spec\n",
        "\n",
        "\treturn model_fn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vgNpXtDFOAOm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dm_single_close_quote = u'\\u2019' # unicode\n",
        "dm_double_close_quote = u'\\u201d'\n",
        "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', dm_single_close_quote, dm_double_close_quote, \")\"] # acceptable ways to end a sentence\n",
        "\n",
        "# These are the number of .story files we expect there to be in cnn_stories_dir and dm_stories_dir\n",
        "num_expected_cnn_stories = 92579\n",
        "num_expected_dm_stories = 219506\n",
        "\n",
        "\n",
        "def read_text_file(text_file):\n",
        "\tlines = []\n",
        "\twith open(text_file, \"r\") as f:\n",
        "\t\tfor line in f:\n",
        "\t\t\tlines.append(line.strip())\n",
        "\treturn lines\n",
        "\n",
        "def check_num_stories(stories_dir, num_expected):\n",
        "\tnum_stories = len(os.listdir(stories_dir))\n",
        "\tif num_stories != num_expected:\n",
        "\t\traise Exception(\"stories directory {} contains {:d} files but should contain {:d}\".format(stories_dir, num_stories, num_expected))\n",
        "\n",
        "def hashhex(s):\n",
        "    \"\"\"Returns a heximal formated SHA1 hash of the input string.\"\"\"\n",
        "    h = hashlib.sha1()\n",
        "    h.update(s.encode())\n",
        "    return h.hexdigest()\n",
        "\n",
        "def get_url_hashes(url_list):\n",
        "\treturn [hashhex(url) for url in url_list]\n",
        "\n",
        "def fix_missing_period(line):\n",
        "\t\"\"\"Adds a period to a line that is missing a period\"\"\"\n",
        "\tif line[-1] in END_TOKENS:\n",
        "\t\treturn line\n",
        "\treturn line + \".\"\n",
        "\n",
        "def get_art_abs(story_file, story_idx):\n",
        "\t# Separate out article and abstract sentences\n",
        "\tstory_text = read_text_file(story_file)\n",
        "\thighlights, article = [], ''\n",
        "\tnext_is_highlight = False\n",
        "\tfor line in story_text:\n",
        "\t\tif line == '':\n",
        "\t\t\tcontinue # empty line\n",
        "\t\telif '@' in line and 'highlight' in line:\n",
        "\t\t\tnext_is_highlight = True\n",
        "\t\telif next_is_highlight:\n",
        "\t\t\thighlights.append(line)\n",
        "\t\t# Put periods on the ends of lines that are missing them\n",
        "\t\t# (this is a problem in the dataset because many image captions don't end in periods;\n",
        "\t\t# consequently they end up in the body of the article as run-on sentences)\n",
        "\t\telse:\n",
        "\t\t\t# Concatenate fixed line to article text\n",
        "\t\t\tarticle += fix_missing_period(line) + ' '\n",
        "\n",
        "\treturn InputExample(story_id=story_idx, article=article, highlights=highlights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kUG34kvyNO3V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FeatureWriter(object):\n",
        "\t\"\"\"Writes InputFeature to TF example file.\"\"\"\n",
        "\n",
        "\tdef __init__(self, filename):\n",
        "\t\tself.filename = filename\n",
        "\t\tself.num_features = 0\n",
        "\t\tself._writer = tf.python_io.TFRecordWriter(filename)\n",
        "\n",
        "\tdef process_feature(self, feature):\n",
        "\t\t\"\"\"Write a InputFeature to the TFRecordWriter as a tf.train.Example.\"\"\"\n",
        "\t\tself.num_features += 1\n",
        "\n",
        "\t\tdef create_int_feature(values):\n",
        "\t\t\tfeature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
        "\t\t\treturn feature\n",
        "\n",
        "\t\tfeatures = collections.OrderedDict()\n",
        "\t\tfeatures[\"unique_ids\"] = create_int_feature([feature.unique_id])\n",
        "\t\tfeatures[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
        "\t\tfeatures[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
        "\t\tfeatures[\"input_type_ids\"] = create_int_feature(feature.input_type_ids)\n",
        "\n",
        "\t\ttf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "\t\tself._writer.write(tf_example.SerializeToString())\n",
        "\n",
        "\tdef close(self):\n",
        "\t\tself._writer.close()\n",
        "\n",
        "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
        "\t\"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
        "\n",
        "\t# Because of the sliding window approach taken to scoring documents, a single\n",
        "\t# token can appear in multiple documents. E.g.\n",
        "\t# \n",
        "\t# Doc: the man went to the store and bought a gallon of milk\n",
        "\t# Span A: the man went to the\n",
        "\t# Span B: to the store and bought\n",
        "\t# Span C: and bought a gallon of\n",
        "\t# ...\n",
        "\t#\n",
        "\t# Now the word 'bought' will have two scores from spans B and C. We only\n",
        "\t# want to consider the score with \"maximum context\", which we define as\n",
        "\t# the *minimum* of its left and right context (the *sum* of left and\n",
        "\t# right context will always be the same, of course).\n",
        "\t#\n",
        "\t# In the example the maximum context for 'bought' would be span C since\n",
        "\t# it has 1 left context and 3 right context, while span B has 4 left context\n",
        "\t# and 0 right context.\n",
        "\tbest_score = None\n",
        "\tbest_span_index = None\n",
        "\tfor (span_index, doc_span) in enumerate(doc_spans):\n",
        "\t\tend = doc_span.start + doc_span.length - 1\n",
        "\t\tif position < doc_span.start:\n",
        "\t\t\tcontinue\n",
        "\t\tif position > end:\n",
        "\t\t\tcontinue\n",
        "\t\tnum_left_context = position - doc_span.start\n",
        "\t\tnum_right_context = end - position\n",
        "\t\tscore = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
        "\t\tif best_score is None or score > best_score:\n",
        "\t\t\tbest_score = score\n",
        "\t\t\tbest_span_index = span_index\n",
        "\n",
        "\treturn cur_span_index == best_span_index\n",
        "\n",
        "def convert_examples_to_features(examples, max_seq_length, batch_size, tokenizer, doc_stride, features_file):\n",
        "\t\"\"\"Splits and tokenizes example stories into lists of length shorter than max_seq_length along with other metadata\"\"\"\n",
        "\teval_writer = FeatureWriter(filename=features_file)\n",
        "\tid_to_features = {}\n",
        "\n",
        "\tunique_id = 1000000000\n",
        "\tfor (example_index, example) in enumerate(examples):\n",
        "\t\t\n",
        "\t\tif example_index % 1000 == 0:\n",
        "\t\t\ttf.logging.info(\"Writing features for example {0:d} of {1:d}; {2:.2f} percent done.\".format(example_index, len(examples), example_index*100.0/len(examples)))\n",
        "\n",
        "\t\t'''# Tokenize highlights\n",
        "\t\tfor highlight_index, highlight in enumerate(example.highlights):\n",
        "\t\t\t\n",
        "\t\t\thighlight_tokens = tokenizer.tokenize(highlight)\n",
        "\t\t\t\n",
        "\t\t\t# Truncate highlight if too long\n",
        "\t\t\t# The -1 accounts for [CLS]\n",
        "\t\t\tif len(highlight_tokens) > max_seq_length -1:\n",
        "\t\t\t\thighlight_tokens = highlight_tokens[0:max_seq_length -1]\n",
        "\n",
        "\t\t\ttokens = []\n",
        "\t\t\ttoken_is_max_context = {}\n",
        "\t\t\tsegment_ids = []\n",
        "\t\t\t\n",
        "\t\t\t# Add classification token\n",
        "\t\t\ttokens.append(\"[CLS]\")\n",
        "\t\t\tsegment_ids.append(0)\n",
        "\n",
        "\t\t\t# Extend with actual highlight tokens\n",
        "\t\t\ttokens.extend(highlight_tokens)\n",
        "\t\t\tsegment_ids.extend([1] * len(highlight_tokens))\n",
        "\n",
        "\t\t\t# Write to temporary features tfrecord\n",
        "\t\t\tfeature = pad_and_create_feature(\n",
        "\t\t\t\t\ttokenizer,\n",
        "\t\t\t\t\twriter=eval_writer,\n",
        "\t\t\t\t\tmax_seq_length=max_seq_length,\n",
        "\t\t\t\t\tunique_id=unique_id,\n",
        "\t\t\t\t\texample_num=example_index,\n",
        "\t\t\t\t\tsection_num=highlight_index,\n",
        "\t\t\t\t\ttokens=tokens,\n",
        "\t\t\t\t\tsegment_ids=segment_ids)\n",
        "\n",
        "\t\t\tid_to_features[unique_id] = feature\n",
        "\t\t\tunique_id += 1\n",
        "\t\t\t'''\n",
        "\t\t# Tokenize article\n",
        "\t\t#tok_to_orig_index = []\n",
        "\t\t#orig_to_tok_index = []\n",
        "\n",
        "\t\tarticle_tokens = tokenizer.tokenize(example.article)\n",
        "\n",
        "\t\t# The -1 accounts for [CLS]\n",
        "\t\tmax_tokens_for_doc = max_seq_length -1\n",
        "\n",
        "\t\t# We can have documents that are longer than the maximum sequence length.\n",
        "\t\t# To deal with this we do a sliding window approach, where we take chunks\n",
        "\t\t# of the up to our max length with a stride of `doc_stride`.\n",
        "\n",
        "\t\t# Calculate article spans\n",
        "\t\t_DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "\t\t\t\t\"DocSpan\", [\"start\", \"length\"])\n",
        "\t\tdoc_spans = []\n",
        "\t\tstart_offset = 0\n",
        "\t\twhile start_offset < len(article_tokens):\n",
        "\t\t\tlength = len(article_tokens) - start_offset\n",
        "\t\t\tif length > max_tokens_for_doc:\n",
        "\t\t\t\tlength = max_tokens_for_doc\n",
        "\t\t\tdoc_spans.append(_DocSpan(start=start_offset, length=length))\n",
        "\t\t\tif start_offset + length == len(article_tokens):\n",
        "\t\t\t\tbreak\n",
        "\t\t\tstart_offset += int(min(length, doc_stride))\n",
        "\n",
        "\n",
        "\t\tfor (article_part_index, doc_span) in enumerate(doc_spans):\n",
        "\t\t\ttokens = []\n",
        "\t\t\ttoken_is_max_context = {}\n",
        "\t\t\tsegment_ids = []\n",
        "\t\t\ttokens.append(\"[CLS]\")\n",
        "\t\t\tsegment_ids.append(0)\n",
        "\n",
        "\t\t\t# Add all article tokens in doc_span's range\n",
        "\t\t\tfor i in range(doc_span.length):\n",
        "\t\t\t\tsplit_token_index = doc_span.start + i\n",
        "\t\t\t\tis_max_context = _check_is_max_context(doc_spans, article_part_index, split_token_index)\n",
        "\t\t\t\ttoken_is_max_context[len(tokens)] = is_max_context\n",
        "\t\t\t\ttokens.append(article_tokens[split_token_index])\n",
        "\t\t\t\tsegment_ids.append(1)\n",
        "\n",
        "\t\t\t# Write to temporary features tfrecord\n",
        "\t\t\tfeature = pad_and_create_feature(\n",
        "\t\t\t\t\ttokenizer,\n",
        "\t\t\t\t\twriter=eval_writer, \n",
        "\t\t\t\t\tmax_seq_length=max_seq_length, \n",
        "\t\t\t\t\tunique_id=unique_id, \n",
        "\t\t\t\t\texample_num=example_index, \n",
        "\t\t\t\t\tsection_num=article_part_index, \n",
        "\t\t\t\t\ttokens=tokens, \n",
        "\t\t\t\t\tsegment_ids=segment_ids,\n",
        "\t\t\t\t\ttoken_is_max_context=token_is_max_context)\n",
        "\n",
        "\t\t\tid_to_features[unique_id] = feature\n",
        "\t\t\tunique_id += 1\n",
        "\n",
        "\ttf.logging.info('Finished writing {1} real features from {2} examples to {0}.'.format(eval_writer.filename, eval_writer.num_features, len(examples)))\n",
        "\t\n",
        "\t# TPU requires a fixed batch size for all batches, therefore the number\n",
        "\t# of features must be a multiple of the batch size, or else features\n",
        "\t# will get dropped. So we pad with fake features which are ignored\n",
        "\t# later on.\n",
        "\twhile eval_writer.num_features % batch_size != 0:\n",
        "\t\tfeature = create_fake_feature(\n",
        "\t\t\t\t\twriter=eval_writer, \n",
        "\t\t\t\t\tmax_seq_length=max_seq_length, \n",
        "\t\t\t\t\tunique_id=unique_id)\n",
        "\t\t\n",
        "\t\tid_to_features[unique_id] = feature\n",
        "\t\tunique_id += 1\n",
        "\n",
        "\ttf.logging.info('Finished writing padding features to {}; should result in {:d} batches.'.format(eval_writer.filename, eval_writer.num_features//batch_size))\n",
        "\t# Close tfrecord writer\n",
        "\teval_writer.close()\n",
        "\treturn id_to_features\n",
        "\n",
        "def pad_and_create_feature(tokenizer, writer, max_seq_length, unique_id, example_num, section_num, tokens, segment_ids, token_is_max_context=None):\n",
        "\t\"\"\"Pads feature data to max_seq_length and writes the resulting feature to a tfRecord file using the `writer`.\"\"\"\n",
        "\t# Convert tokens from tokenizer to ids from vocab file\n",
        "\tinput_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\t\n",
        "\t# The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "\t# tokens are attended to.\n",
        "\tinput_mask = [1] * len(input_ids)\n",
        "\n",
        "\t# Zero-pad up to the sequence length.\n",
        "\twhile len(input_ids) < max_seq_length:\n",
        "\t\tinput_ids.append(0)\n",
        "\t\tinput_mask.append(0)\n",
        "\t\tsegment_ids.append(0)\n",
        "\n",
        "\tassert len(input_ids) == max_seq_length\n",
        "\tassert len(input_mask) == max_seq_length\n",
        "\tassert len(segment_ids) == max_seq_length\n",
        "\n",
        "\tif example_num < 2:\n",
        "\t\ttf.logging.info(\"*** Example ***\")\n",
        "\t\ttf.logging.info(\"unique_id: {}\".format(unique_id))\n",
        "\t\ttf.logging.info(\"example_num: {}\".format(example_num))\n",
        "\t\ttf.logging.info(\"section_num: {}\".format(section_num))\n",
        "\t\t\n",
        "\t\ttf.logging.info(\"tokens: {}\".format(''.join(\n",
        "\t\t\t\t[tokenization.printable_text(x) + ' ' for x in tokens])))\n",
        "\t\tif token_is_max_context:\n",
        "\t\t\ttf.logging.info(\"token_is_max_context: {}\".format(''.join(\n",
        "\t\t\t\t[\"{0}:{1}\".format(x, y) + ', ' for (x, y) in token_is_max_context.items()])))\n",
        "\n",
        "\t\ttf.logging.info(\"input_ids: {}\".format(''.join([str(x) + ' ' for x in input_ids])))\n",
        "\t\ttf.logging.info(\n",
        "\t\t\t\t\"input_mask: {}\".format(''.join([str(x) + ' ' for x in input_mask])))\n",
        "\t\ttf.logging.info(\n",
        "\t\t\t\t\"segment_ids: {}\".format(''.join([str(x) + ' ' for x in segment_ids])))\n",
        "\n",
        "\tfeature = InputFeatures(\n",
        "\t\t\tunique_id=unique_id,\n",
        "\t\t\texample_id=example_num,\n",
        "\t\t\tdoc_span_index=section_num,\n",
        "\t\t\ttokens=tokens,\n",
        "\t\t\ttoken_is_max_context=token_is_max_context,\n",
        "\t\t\tinput_ids=input_ids,\n",
        "\t\t\tinput_mask=input_mask,\n",
        "\t\t\tinput_type_ids=segment_ids)\n",
        "\n",
        "\t# Write to temporary features tfrecord\n",
        "\twriter.process_feature(feature)\n",
        "\treturn feature\n",
        "\n",
        "def create_fake_feature(writer, max_seq_length, unique_id):\n",
        "\t\"\"\"Pads feature data to max_seq_length and writes the resulting feature to a tfRecord file using the `writer`.\"\"\"\n",
        "\t\n",
        "\t# The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "\t# tokens are attended to.\n",
        "\tfeature = InputFeatures(\n",
        "\t\t\tunique_id=unique_id,\n",
        "\t\t\texample_id=None,\n",
        "\t\t\tdoc_span_index=None,\n",
        "\t\t\ttokens=[None] * max_seq_length,\n",
        "\t\t\ttoken_is_max_context=[False] * max_seq_length,\n",
        "\t\t\tinput_ids=[0] * max_seq_length,\n",
        "\t\t\tinput_mask=[0] * max_seq_length,\n",
        "\t\t\tinput_type_ids=[0] * max_seq_length,\n",
        "\t\t\tis_real=False)\n",
        "\n",
        "\t# Write to temporary features tfrecord\n",
        "\twriter.process_feature(feature)\n",
        "\treturn feature\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fQj4rFV7NaX-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BERT_CONFIG = BERT_PRETRAINED_DIR +'/bert_config.json'\n",
        "BERT_VOCAB = BERT_PRETRAINED_DIR +'/vocab.txt'\n",
        "BERT_CHECKPOINT = BERT_PRETRAINED_DIR +'/bert_model.ckpt'\n",
        "\n",
        "MAX_SEQ_LEN = 384\n",
        "BATCH_SIZE = 8\n",
        "NUM_TPU_CORES = 8\n",
        "\n",
        "layer_indexes = [-1]\n",
        "\n",
        "def write_to_bin(cnn_stories_dir, dm_stories_dir, url_filename, out_filename, temp_records_filename):\n",
        "\tout_file = os.fsencode(os.path.join(FINISHED_DIR, out_filename))\n",
        "\tfeatures_file = os.fsencode(os.path.join(FINISHED_DIR, temp_records_filename))\n",
        "\n",
        "\t\"\"\"Reads the tokenized .story files corresponding to the urls listed in the url_file and writes them to a out_file.\"\"\"\n",
        "\t# print(\"Making bin file for URLs listed in {}...\".format(url_file))\n",
        "\turl_list = read_text_file(os.fsencode(os.path.join(URL_DIR, url_filename)))\n",
        "\turl_hashes = get_url_hashes(url_list)\n",
        "\tstory_fnames = [s + \".story\" for s in url_hashes]\n",
        "\n",
        "\tnum_stories = len(story_fnames)\n",
        "\tprint(\"Found {:d} stories listed in URL list...\".format(num_stories))\n",
        "\n",
        "\t# Instantiate Tokenizer and BERT Model from checkpoint\n",
        "\tbert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)\n",
        "\tprint(\"Found BERT config file at {}...\".format(BERT_CONFIG))\n",
        "\n",
        "\ttokenizer = tokenization.FullTokenizer(vocab_file=BERT_VOCAB, do_lower_case=False)\n",
        "\tprint(\"Instantiated BERT Tokenizer with vocab file from {}...\".format(BERT_VOCAB))\n",
        "\n",
        "\tis_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "\trun_config = tf.contrib.tpu.RunConfig(master=TPU_ADDRESS,\n",
        "\t\t\ttpu_config=tf.contrib.tpu.TPUConfig(\n",
        "\t\t\t\t\tnum_shards=NUM_TPU_CORES,\n",
        "\t\t\t\t\tper_host_input_for_training=is_per_host))\n",
        "\n",
        "\tmodel_fn = model_fn_builder(\n",
        "\t\t\tbert_config=bert_config,\n",
        "\t\t\tinit_checkpoint=BERT_CHECKPOINT,\n",
        "\t\t\tlayer_indexes=layer_indexes,\n",
        "\t\t\tuse_tpu=True,\n",
        "\t\t\tuse_one_hot_embeddings=True)\n",
        "\n",
        "\t# If TPU is not available, this will fall back to normal Estimator on CPU\n",
        "\t# or GPU.\n",
        "\testimator = tf.contrib.tpu.TPUEstimator(\n",
        "\t\t\tuse_tpu=True,\n",
        "\t\t\tmodel_fn=model_fn,\n",
        "\t\t\tconfig=run_config,\n",
        "\t\t\ttrain_batch_size=BATCH_SIZE,\n",
        "\t\t\tpredict_batch_size=BATCH_SIZE)\n",
        "\n",
        "\tprint(\"Successfully set up TPU estimator with BERT mode from checkpoint at {} with batch size {}...\".format(BERT_CHECKPOINT, BATCH_SIZE))\n",
        "\texamples = []\n",
        "\tfor idx,s in enumerate(story_fnames):\n",
        "\t\tif idx % 1000 == 0:\n",
        "\t\t\tprint(\"Reading story {0:d} of {1:d}; {2:.2f} percent done\".format(idx, num_stories, float(idx)*100.0/float(num_stories)))\n",
        "\n",
        "\t\t# Look in the tokenized story dirs to find the .story file corresponding to this url\n",
        "\t\tif os.path.isfile(os.path.join(cnn_stories_dir, s)):\n",
        "\t\t\tstory_file = os.path.join(cnn_stories_dir, s)\n",
        "\t\telif os.path.isfile(os.path.join(dm_stories_dir, s)):\n",
        "\t\t\tstory_file = os.path.join(dm_stories_dir, s)\n",
        "\t\telse:\n",
        "\t\t\tprint(\"Error: Couldn't find story file {} in either story directories {} and {}. Are the directories misplaced or missing?\".format(s, cnn_stories_dir, dm_stories_dir))\n",
        "\t\t\t# Check again if tokenized stories directories contain correct number of files\n",
        "\t\t\tprint(\"Checking that the stories directories {} and {} contain correct number of files...\".format(cnn_stories_dir, dm_stories_dir))\n",
        "\t\t\tcheck_num_stories(cnn_stories_dir, num_expected_cnn_stories)\n",
        "\t\t\tcheck_num_stories(dm_stories_dir, num_expected_dm_stories)\n",
        "\t\t\traise Exception(\"Stories directories {} and {} contain correct number of files but story file {} found in neither.\".format(cnn_stories_dir, dm_stories_dir, s))\n",
        "\n",
        "\t\t# Get the example to tokenize from story file\n",
        "\t\texamples.append(get_art_abs(story_file, idx))\n",
        "\t\t\n",
        "\tunique_id_to_feature = convert_examples_to_features(\n",
        "\t\texamples=examples,\n",
        "\t\tmax_seq_length=MAX_SEQ_LEN,\n",
        "\t\tbatch_size=BATCH_SIZE,\n",
        "\t\ttokenizer=tokenizer,\n",
        "\t\tdoc_stride=MAX_SEQ_LEN//3,\n",
        "\t\tfeatures_file=features_file)\n",
        "\t\n",
        "\tinput_fn = file_based_input_fn_builder(\n",
        "\t\tinput_file=features_file,\n",
        "\t\tseq_length=MAX_SEQ_LEN,\n",
        "\t\tnum_cores=NUM_TPU_CORES)\n",
        "\n",
        "\ttf.logging.info(\"***** Running predictions *****\")\n",
        "\ttf.logging.info(\"  Num orig examples: {:d}\".format(len(examples)))\n",
        "\ttf.logging.info(\"  Num total split sentences: {:d}\".format(len(unique_id_to_feature)))\n",
        "\ttf.logging.info(\"  Batch size per TPU group: {:d}\".format(BATCH_SIZE))\n",
        "\n",
        "\t# TODO: Complete convert_examples_to_features\n",
        "\t# TODO: Write results to tfrecord to feed into decoder\n",
        "\twith codecs.getwriter(\"utf-8\")(tf.gfile.Open(out_file), \"w\") as writer:\n",
        "\t\tfor result in estimator.predict(input_fn, yield_single_examples=True):\n",
        "\t\t\tunique_id = int(result[\"unique_id\"])\n",
        "\t\t\ttf.logging.info(\"Received result with unique id {:d}\".format(out_file))\n",
        "\t\t\tfeature = unique_id_to_feature[unique_id]\n",
        "\t\t\t\n",
        "\t\t\t# Skip fake features used for padding\n",
        "\t\t\tif not feature.is_real:\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\toutput_json = collections.OrderedDict()\n",
        "\t\t\toutput_json[\"linex_index\"] = unique_id\n",
        "\t\t\toutput_json[\"example_id\"] = feature.example_id\n",
        "\t\t\toutput_json[\"doc_span_index\"] = feature.doc_span_index\n",
        "\t\t\tall_features = []\n",
        "\t\t\tfor (i, token) in enumerate(feature.tokens):\n",
        "\t\t\t\tall_layers = []\n",
        "\t\t\t\tfor (j, layer_index) in enumerate(layer_indexes):\n",
        "\t\t\t\t\tlayer_output = result[\"layer_output_{:d}\".format(j)]\n",
        "\t\t\t\t\tlayers = collections.OrderedDict()\n",
        "\t\t\t\t\tlayers[\"index\"] = layer_index\n",
        "\t\t\t\t\tlayers[\"values\"] = [\n",
        "\t\t\t\t\t\t\tround(float(x), 6) for x in layer_output[i:(i + 1)].flat\n",
        "\t\t\t\t\t]\n",
        "\t\t\t\t\tall_layers.append(layers)\n",
        "\t\t\t\tfeatures = collections.OrderedDict()\n",
        "\t\t\t\tfeatures[\"token\"] = token\n",
        "\t\t\t\tfeatures[\"layers\"] = all_layers\n",
        "\t\t\t\tall_features.append(features)\n",
        "\t\t\toutput_json[\"features\"] = all_features\n",
        "\t\t\twriter.write(json.dumps(output_json) + \"\\n\")\n",
        "\t\n",
        "\t\tprint(\"Finished writing file {}\\n\".format(out_file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9CNrXorDOU_u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CHUNK_SIZE = 1000 # num examples per chunk, for the chunked data\n",
        "\n",
        "def chunk_file(set_name):\n",
        "\tin_file = 'finished_files/{}.jsonl'.format(set_name)\n",
        "\treader = open(in_file, \"rb\")\n",
        "\tchunk = 0\n",
        "\tfinished = False\n",
        "\twhile not finished:\n",
        "\t\tchunk_fname = os.path.join(CHUNKED_DIR, '{0}_{1:03d}.jsonl'.format(set_name, chunk)) # new chunk\n",
        "\t\twith open(chunk_fname, 'wb') as writer:\n",
        "\t\t\tfor _ in range(CHUNK_SIZE):\n",
        "\t\t\t\tlen_bytes = reader.read(8)\n",
        "\t\t\t\tif not len_bytes:\n",
        "\t\t\t\t\tfinished = True\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\t\tstr_len = struct.unpack('q', len_bytes)[0]\n",
        "\t\t\t\texample_str = struct.unpack('{:d}s'.format(str_len), reader.read(str_len))[0]\n",
        "\t\t\t\twriter.write(struct.pack('q', str_len))\n",
        "\t\t\t\twriter.write(struct.pack('{:d}s'.format(str_len), example_str))\n",
        "\t\t\tchunk += 1\n",
        "\n",
        "def chunk_all(chunkdir):\n",
        "\t# Make a dir to hold the chunks\n",
        "\tif not os.path.isdir(chunkdir):\n",
        "\t\tos.mkdir(chunkdir)\n",
        "\t# Chunk the data\n",
        "\tfor set_name in ['train', 'val', 'test']:\n",
        "\t\tprint(\"Splitting {} data into chunks...\".format(set_name))\n",
        "\t\tchunk_file(set_name)\n",
        "\tprint(\"Saved chunked data in {}\".format(chunkdir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "227j6-gkNO-9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Check the stories directories contain the correct number of .story files\n",
        "check_num_stories(CNN_DIR + 'stories/', num_expected_cnn_stories)\n",
        "check_num_stories(DAILYMAIL_DIR + 'stories/', num_expected_dm_stories)\n",
        "\n",
        "# Create some new directories\n",
        "if not os.path.exists(FINISHED_DIR):\n",
        "    os.makedirs(FINISHED_DIR)\n",
        "\n",
        "# Run BERT Tokenizer on both stories dirs, then read the tokenized stories, do a little postprocessing then write to bin files\n",
        "for set_name in ['train', 'val', 'test']:\n",
        "\twrite_to_bin(CNN_DIR + 'stories/', DAILYMAIL_DIR + 'stories/', \"all_{}.txt\".format(set_name), \"{}.jsonl\".format(set_name), \"temp_{}.tfrecord\".format(set_name))\n",
        "\n",
        "# TODO: Chunk the data. This splits each of train.jsonl, val.jsonl and test.jsonl into smaller chunks, each containing e.g. 1000 examples, and saves them in FINISHED_DIR/chunks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KIYJZj9RvTHZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-dta1G8rTlRE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cat cnn/stories/1a87358e123db0ecdaeebb3a675b73ea1ad97635.story"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TCBtcpZkykSf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Construct a Simple Decoder\n",
        "\n",
        "Use the trained model to make predictions and generate your own Shakespeare-esque play.\n",
        "Start the model off with a *seed* sentence, then generate 250 characters from it. We'll make five predictions from the initial seed."
      ]
    },
    {
      "metadata": {
        "id": "eoXRtSPZvdiS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Eval the model.\n",
        "eval_examples = processor.get_dev_examples(TASK_DATA_DIR)\n",
        "eval_features = run_classifier.convert_examples_to_features(\n",
        "    eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "print('***** Started evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "print('  Num examples = {}'.format(len(eval_examples)))\n",
        "print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
        "# Eval will be slightly WRONG on the TPU because it will truncate\n",
        "# the last batch.\n",
        "eval_steps = int(len(eval_examples) / EVAL_BATCH_SIZE)\n",
        "eval_input_fn = run_classifier.input_fn_builder(\n",
        "    features=eval_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=True)\n",
        "result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "output_eval_file = os.path.join(OUTPUT_DIR, \"eval_results.txt\")\n",
        "with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "    print(\"***** Eval results *****\")\n",
        "    for key in sorted(result.keys()):\n",
        "        print('  {} = {}'.format(key, str(result[key])))\n",
        "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "X5Igcc6wW6sK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train the model.\n",
        "print('Shakespeare on BERT base model normally takes about ??? minutes. Please wait...')\n",
        "train_features = run_classifier.convert_examples_to_features(\n",
        "    train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "print('  Num examples = {}'.format(len(train_examples)))\n",
        "print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "train_input_fn = run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=True)\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print('***** Finished training at {} *****'.format(datetime.datetime.now()))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}